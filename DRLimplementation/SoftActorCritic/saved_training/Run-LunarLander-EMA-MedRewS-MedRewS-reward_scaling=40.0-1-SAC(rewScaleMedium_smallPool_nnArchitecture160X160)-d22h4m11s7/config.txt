   ExperimentSpec{
      'algo_name': Soft Actor Critic
      'comment': rewScaleMedium smallPool nnArchitecture160X160
      'paramameter_set_name': SAC
      'rerun_tag': LunarLander-EMA-MedRewS-MedRewS-reward_scaling=40.0
      'rerun_idx': 1
      'isTestRun': False
      'prefered_environment': LunarLanderContinuous-v2
      'expected_reward_goal': 190
      'show_plot': False
      'batch_size_in_ts': 200
      'max_epoch': 25
      'discout_factor': 0.99
      'learning_rate': 0.003
      'discounted_reward_to_go': True
      'theta_nn_h_layer_topo': (160, 160)
      'random_seed': 0
      'theta_hidden_layers_activation': <function relu at 0x12fee0510>
      'theta_output_layers_activation': None
      'render_env_every_What_epoch': 5
      'log_metric_interval': 500
      'print_metric_every_what_epoch': 5
      'max_trj_steps': 1000
      'AgentType': <class 'SoftActorCritic.SoftActorCriticAgent.SoftActorCriticAgent'>
      'timestep_per_epoch': 5000
      'reward_scaling': 40.0
      'critic_learning_rate': 0.003
      'max_gradient_step_expected': 125000
      'actor_lr_decay_rate': 1.0
      'critic_lr_decay_rate': 1.0
      'target_smoothing_coefficient': 0.005
      'target_update_interval': 1
      'gradient_step_interval': 1
      'alpha': 1.0
      'max_eval_trj': 10
      'pool_capacity': 10000
      'min_pool_size': 5000
      'phi_nn_h_layer_topo': (160, 160)
      'phi_hidden_layers_activation': <function relu at 0x12fee0510>
      'phi_output_layers_activation': None
      'psi_nn_h_layer_topo': (160, 160)
      'psi_hidden_layers_activation': <function relu at 0x12fee0510>
      'psi_output_layers_activation': None
      'render_env_eval_interval': 5
      'note': New hypothesis: based on Mujoco/SAC paper experiment,    the mean-heated-rew ~ [19 < x < 30]Base on closer Box[3,] Hopper, our meanHeatedRewGoal ~ 19So we seek: rew_thr / maxStep * rewS ~ 19 1000*19/200/ = 95 = rewS_lunar
      'SpinningUpSquashing': False
      'SpinningUpGaussian': False
   }